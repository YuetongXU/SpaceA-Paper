{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"] = 42\n",
    "import scanpy as sc\n",
    "sys.path.append('/home/goubo/spt')\n",
    "import assembly\n",
    "\n",
    "sc.settings.verbosity = 0  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.set_figure_params(scanpy=True, dpi=100, dpi_save=300, facecolor='white', frameon=True, vector_friendly=True, \n",
    "                     fontsize=12,\n",
    "                    #  figsize=(4,4),\n",
    "                     color_map=None, format='png', transparent=False, ipython_format='png2x')\n",
    "sc.settings.n_jobs=1\n",
    "sc.settings.figdir = \"./\"\n",
    "\n",
    "sys.path.append('/home/goubo/GitHub/3Dgenome/Higashi-main')\n",
    "sys.path.append('/home/goubo/GitHub/3Dgenome/Fast-Higashi-main')\n",
    "from fasthigashi.FastHigashi_Wrapper import *\n",
    "from higashi.Higashi_wrapper import *\n",
    "\n",
    "species='mm10'\n",
    "chroms=list(assembly.build(species, 1)._chromsizes.keys())\n",
    "work_dir='/home/spaceA/higashi/fasthigashi'\n",
    "cpu_num=60\n",
    "filter_spot=False\n",
    "umap_n_neighbors=20\n",
    "tolerance= 2e-5 \n",
    "restore_order=True\n",
    "do_conv=True\n",
    "do_rwr=False\n",
    "embed_type='embed_l2_norm'\n",
    "\n",
    "with open(\"/home/spaceA/config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_name):\n",
    "    print(f'\\nNow process:{sample_name}')\n",
    "    out_dir=os.path.join(work_dir,sample_name)\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "    os.chdir(out_dir)\n",
    "\n",
    "    ## 1. Prepare the hagashi input of config_hagashi\n",
    "    config_hagashi = os.path.join(out_dir,'config_higashi_v1.JSON')\n",
    "    template_config =os.path.join(work_dir,'config_higashi_template_v1.JSON')\n",
    "    with open(template_config, 'r') as f:\n",
    "        config_template = json.load(f)\n",
    "        config_template['config_name']=sample_name\n",
    "        config_template['data_dir']=out_dir\n",
    "        config_template['temp_dir']=os.path.join(out_dir,'temp_fasthigashi')\n",
    "        config_template['input_format']=\"higashi\"\n",
    "        config_template['header_included']=False\n",
    "        config_template['cpu_num']=cpu_num\n",
    "        config_template['contact_header']=[\"chrom1\", \"pos1\", \"chrom2\", \"pos2\", \"count\"]\n",
    "        config_template['plot_label']='spot_cluster'\n",
    "        config_template['neighbor_num']=5\n",
    "        config_template['UMAP_params']['n_neighbors']=umap_n_neighbors\n",
    "        config_template['optional_smooth']=False\n",
    "        config_template['chrom_list']=[x for x in chroms if x not in ['chrM','chrX','chrY']]\n",
    "        config_template['genome_reference_path']=\"/home/mm10.mainchr.sizes\"\n",
    "        config_template['cytoband_path']=\"/home/spaceA/fasthigashi/mm10_cytoBand.txt\"\n",
    "\n",
    "    with open(config_hagashi, 'w') as file:\n",
    "            json.dump(config_template, file, indent=4)\n",
    "\n",
    "    ## 2. prepare the hagashi input of leiden label_info from other cluster method not hagashi\n",
    "    spot_infor_f=os.path.join('/home/spaceA/SpatialSPRITE_res/Filter_Spot_v4_tmp',\n",
    "                                f'{sample_name}_spot_infor_final.csv')\n",
    "    spot_infor=pd.read_csv(spot_infor_f)\n",
    "    spot_cluster_infor_f=os.path.join('/home/spaceA/SpatialSPRITE_res/Leiden.0',\n",
    "                            sample_name+'_obs.csv')\n",
    "    spot_cluster_infor=pd.read_csv(spot_cluster_infor_f,index_col=0)\n",
    "\n",
    "    spot_infor_merge=spot_infor.merge(spot_cluster_infor,left_on='spot_id',right_on='spotid',how='left')\n",
    "    assigned_clusters=spot_infor_merge['leiden'].apply(lambda x: 'cluster_'+ str(x)).to_list()\n",
    "\n",
    "    label_info_new={'spot_cluster':assigned_clusters}\n",
    "    with open ('label_info.pickle','wb') as f:\n",
    "        pickle.dump(label_info_new, f)\n",
    "\n",
    "    ## 3. Run the Fast-Higashi model on its own or as an initialization for Higashi\n",
    "    ### 3.1 Initialize Fast-Higashi model and turn sparse matrices into sparse tensors\n",
    "    fh_model = FastHigashi(config_path=config_hagashi,\n",
    "                        path2input_cache=None, # when setting at None, will use the temp_dir on the JSON file\n",
    "                        path2result_dir=None, # same as above\n",
    "                        off_diag=100,\n",
    "                        filter=filter_spot,\n",
    "                        do_conv=do_conv, # at coarser resolution for high cov data, recommend to be False\n",
    "                        do_rwr=do_rwr, # For high-cov data, the differences are minor, will show later with do_rwr=True option\n",
    "                        do_col=False,\n",
    "                        no_col=False)\n",
    "\n",
    "\n",
    "    # config_path           The path to the configuration JSON file that you created.\n",
    "    # path2input_cache      The path to the directory where the cached tensor file will be stored\n",
    "    # path2result_dir       The path to the directory where the cached tensor file will be stored\n",
    "    # off_diag              Maximum No of diagonals to consider. When set as 100, the 0-100th diagonal would \n",
    "    #                       be considered\n",
    "    # filter                Whether only use cells that pass the quality control standard to learn the meta-interactions, \n",
    "    #                       and then infers the embeddings for the result of the cells. \n",
    "    # do_conv               Whether use linear convolution or not.\n",
    "    # do_rwr                Whether use partial random walk with restart or not\n",
    "    # do_col                Whether use sqrt_vc normalization or not, the program \n",
    "    #                       would automatically uses it when needed\n",
    "    # no_col                Whether force the program to not use sqrt_vc normalization, the program would automatically uses it when needed\n",
    "    # batch_norm            Whether uses batch corrected normalization or not\n",
    "\n",
    "    # From contact pairs to the sparse matrices and store them on disk\n",
    "    if not os.path.exists(os.path.join(fh_model.temp_dir, \"raw\", \"%s_sparse_adj.npy\" % fh_model.chrom_list[0])):\n",
    "        start = time.time()\n",
    "        fh_model.fast_process_data()\n",
    "        print(\"contact pairs to sparse mtx takes: %.2f s\" % (time.time() - start))\n",
    "        \n",
    "    # packing data from sparse matrices to sparse tensors\n",
    "    start = time.time()\n",
    "    fh_model.prep_dataset(batch_norm=False) # we don't have batch_id provided so, set as False\n",
    "    print(\"packing sparse mtx takes: %.2f s\" % (time.time() - start))\n",
    "\n",
    "    fh_model.run_model(extra=\"\", # can be any words, this will be appended to the model name when the model is saved. Used as an identifier.\n",
    "                    rank=256,\n",
    "                    n_iter_parafac=1,\n",
    "                    tol=tolerance #3e-4 # In the original manuscript, we use this tolerance, but later we found that setting it to smaller ones might lead to better performance on some data. Will do an ablation later\n",
    "                    )\n",
    "\n",
    "    # loading existing trained models\n",
    "    # This operation is optional when the model is just trained\n",
    "    fh_model.load_model(extra=\"\",rank=256,n_iter_parafac=1)\n",
    "\n",
    "    # getting embedding\n",
    "    embed = fh_model.fetch_cell_embedding(final_dim=256,\n",
    "                                        restore_order=restore_order)\n",
    "    # The returned embed is a dictionary that stores the embeddings after different ways of post-processing.\n",
    "    # 'embed_l2_norm' or 'embed_l2_norm_correct_coverage_fh' usually yields the best results, the latter one represents linear correction of sequencing depth bias.\n",
    "\n",
    "    # save embedding\n",
    "    embed_f='embed.pickle'\n",
    "    with open (embed_f,'wb') as f:\n",
    "        pickle.dump(embed, f)\n",
    "\n",
    "    print(f'{sample_name} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampleid_list= config['spatial_infor'].keys() \n",
    "for sample in tqdm(sampleid_list):\n",
    "    main(sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
